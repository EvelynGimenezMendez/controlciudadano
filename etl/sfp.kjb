<?xml version="1.0" encoding="UTF-8"?>
<job>
  <name>sfp</name>
  <description/>
  <extended_description/>
  <job_version/>
  <job_status>1</job_status>
  <directory>/</directory>
  <created_user>-</created_user>
  <created_date>2020/08/15 20:20:30.512</created_date>
  <modified_user>-</modified_user>
  <modified_date>2020/08/15 20:20:30.512</modified_date>
  <parameters>
    <parameter>
      <name>ANO</name>
      <default_value>2020</default_value>
      <description>Ano a bajar</description>
    </parameter>
    <parameter>
      <name>MES</name>
      <default_value>4</default_value>
      <description>Mes a bajar</description>
    </parameter>
  </parameters>
  <connection>
    <name>maindb</name>
    <server>a</server>
    <type>POSTGRESQL</type>
    <access>Native</access>
    <database>a</database>
    <port/>
    <username>postgres</username>
    <password>Encrypted </password>
    <servername/>
    <data_tablespace/>
    <index_tablespace/>
    <attributes>
      <attribute>
        <code>FORCE_IDENTIFIERS_TO_LOWERCASE</code>
        <attribute>N</attribute>
      </attribute>
      <attribute>
        <code>FORCE_IDENTIFIERS_TO_UPPERCASE</code>
        <attribute>N</attribute>
      </attribute>
      <attribute>
        <code>IS_CLUSTERED</code>
        <attribute>N</attribute>
      </attribute>
      <attribute>
        <code>PRESERVE_RESERVED_WORD_CASE</code>
        <attribute>Y</attribute>
      </attribute>
      <attribute>
        <code>QUOTE_ALL_FIELDS</code>
        <attribute>N</attribute>
      </attribute>
      <attribute>
        <code>SUPPORTS_BOOLEAN_DATA_TYPE</code>
        <attribute>N</attribute>
      </attribute>
      <attribute>
        <code>SUPPORTS_TIMESTAMP_DATA_TYPE</code>
        <attribute>N</attribute>
      </attribute>
      <attribute>
        <code>USE_POOLING</code>
        <attribute>N</attribute>
      </attribute>
    </attributes>
  </connection>
  <slaveservers>
    </slaveservers>
  <job-log-table>
    <connection/>
    <schema/>
    <table/>
    <size_limit_lines/>
    <interval/>
    <timeout_days/>
    <field>
      <id>ID_JOB</id>
      <enabled>Y</enabled>
      <name>ID_JOB</name>
    </field>
    <field>
      <id>CHANNEL_ID</id>
      <enabled>Y</enabled>
      <name>CHANNEL_ID</name>
    </field>
    <field>
      <id>JOBNAME</id>
      <enabled>Y</enabled>
      <name>JOBNAME</name>
    </field>
    <field>
      <id>STATUS</id>
      <enabled>Y</enabled>
      <name>STATUS</name>
    </field>
    <field>
      <id>LINES_READ</id>
      <enabled>Y</enabled>
      <name>LINES_READ</name>
    </field>
    <field>
      <id>LINES_WRITTEN</id>
      <enabled>Y</enabled>
      <name>LINES_WRITTEN</name>
    </field>
    <field>
      <id>LINES_UPDATED</id>
      <enabled>Y</enabled>
      <name>LINES_UPDATED</name>
    </field>
    <field>
      <id>LINES_INPUT</id>
      <enabled>Y</enabled>
      <name>LINES_INPUT</name>
    </field>
    <field>
      <id>LINES_OUTPUT</id>
      <enabled>Y</enabled>
      <name>LINES_OUTPUT</name>
    </field>
    <field>
      <id>LINES_REJECTED</id>
      <enabled>Y</enabled>
      <name>LINES_REJECTED</name>
    </field>
    <field>
      <id>ERRORS</id>
      <enabled>Y</enabled>
      <name>ERRORS</name>
    </field>
    <field>
      <id>STARTDATE</id>
      <enabled>Y</enabled>
      <name>STARTDATE</name>
    </field>
    <field>
      <id>ENDDATE</id>
      <enabled>Y</enabled>
      <name>ENDDATE</name>
    </field>
    <field>
      <id>LOGDATE</id>
      <enabled>Y</enabled>
      <name>LOGDATE</name>
    </field>
    <field>
      <id>DEPDATE</id>
      <enabled>Y</enabled>
      <name>DEPDATE</name>
    </field>
    <field>
      <id>REPLAYDATE</id>
      <enabled>Y</enabled>
      <name>REPLAYDATE</name>
    </field>
    <field>
      <id>LOG_FIELD</id>
      <enabled>Y</enabled>
      <name>LOG_FIELD</name>
    </field>
    <field>
      <id>EXECUTING_SERVER</id>
      <enabled>N</enabled>
      <name>EXECUTING_SERVER</name>
    </field>
    <field>
      <id>EXECUTING_USER</id>
      <enabled>N</enabled>
      <name>EXECUTING_USER</name>
    </field>
    <field>
      <id>START_JOB_ENTRY</id>
      <enabled>N</enabled>
      <name>START_JOB_ENTRY</name>
    </field>
    <field>
      <id>CLIENT</id>
      <enabled>N</enabled>
      <name>CLIENT</name>
    </field>
  </job-log-table>
  <jobentry-log-table>
    <connection/>
    <schema/>
    <table/>
    <timeout_days/>
    <field>
      <id>ID_BATCH</id>
      <enabled>Y</enabled>
      <name>ID_BATCH</name>
    </field>
    <field>
      <id>CHANNEL_ID</id>
      <enabled>Y</enabled>
      <name>CHANNEL_ID</name>
    </field>
    <field>
      <id>LOG_DATE</id>
      <enabled>Y</enabled>
      <name>LOG_DATE</name>
    </field>
    <field>
      <id>JOBNAME</id>
      <enabled>Y</enabled>
      <name>TRANSNAME</name>
    </field>
    <field>
      <id>JOBENTRYNAME</id>
      <enabled>Y</enabled>
      <name>STEPNAME</name>
    </field>
    <field>
      <id>LINES_READ</id>
      <enabled>Y</enabled>
      <name>LINES_READ</name>
    </field>
    <field>
      <id>LINES_WRITTEN</id>
      <enabled>Y</enabled>
      <name>LINES_WRITTEN</name>
    </field>
    <field>
      <id>LINES_UPDATED</id>
      <enabled>Y</enabled>
      <name>LINES_UPDATED</name>
    </field>
    <field>
      <id>LINES_INPUT</id>
      <enabled>Y</enabled>
      <name>LINES_INPUT</name>
    </field>
    <field>
      <id>LINES_OUTPUT</id>
      <enabled>Y</enabled>
      <name>LINES_OUTPUT</name>
    </field>
    <field>
      <id>LINES_REJECTED</id>
      <enabled>Y</enabled>
      <name>LINES_REJECTED</name>
    </field>
    <field>
      <id>ERRORS</id>
      <enabled>Y</enabled>
      <name>ERRORS</name>
    </field>
    <field>
      <id>RESULT</id>
      <enabled>Y</enabled>
      <name>RESULT</name>
    </field>
    <field>
      <id>NR_RESULT_ROWS</id>
      <enabled>Y</enabled>
      <name>NR_RESULT_ROWS</name>
    </field>
    <field>
      <id>NR_RESULT_FILES</id>
      <enabled>Y</enabled>
      <name>NR_RESULT_FILES</name>
    </field>
    <field>
      <id>LOG_FIELD</id>
      <enabled>N</enabled>
      <name>LOG_FIELD</name>
    </field>
    <field>
      <id>COPY_NR</id>
      <enabled>N</enabled>
      <name>COPY_NR</name>
    </field>
  </jobentry-log-table>
  <channel-log-table>
    <connection/>
    <schema/>
    <table/>
    <timeout_days/>
    <field>
      <id>ID_BATCH</id>
      <enabled>Y</enabled>
      <name>ID_BATCH</name>
    </field>
    <field>
      <id>CHANNEL_ID</id>
      <enabled>Y</enabled>
      <name>CHANNEL_ID</name>
    </field>
    <field>
      <id>LOG_DATE</id>
      <enabled>Y</enabled>
      <name>LOG_DATE</name>
    </field>
    <field>
      <id>LOGGING_OBJECT_TYPE</id>
      <enabled>Y</enabled>
      <name>LOGGING_OBJECT_TYPE</name>
    </field>
    <field>
      <id>OBJECT_NAME</id>
      <enabled>Y</enabled>
      <name>OBJECT_NAME</name>
    </field>
    <field>
      <id>OBJECT_COPY</id>
      <enabled>Y</enabled>
      <name>OBJECT_COPY</name>
    </field>
    <field>
      <id>REPOSITORY_DIRECTORY</id>
      <enabled>Y</enabled>
      <name>REPOSITORY_DIRECTORY</name>
    </field>
    <field>
      <id>FILENAME</id>
      <enabled>Y</enabled>
      <name>FILENAME</name>
    </field>
    <field>
      <id>OBJECT_ID</id>
      <enabled>Y</enabled>
      <name>OBJECT_ID</name>
    </field>
    <field>
      <id>OBJECT_REVISION</id>
      <enabled>Y</enabled>
      <name>OBJECT_REVISION</name>
    </field>
    <field>
      <id>PARENT_CHANNEL_ID</id>
      <enabled>Y</enabled>
      <name>PARENT_CHANNEL_ID</name>
    </field>
    <field>
      <id>ROOT_CHANNEL_ID</id>
      <enabled>Y</enabled>
      <name>ROOT_CHANNEL_ID</name>
    </field>
  </channel-log-table>
  <pass_batchid>N</pass_batchid>
  <shared_objects_file/>
  <entries>
    <entry>
      <name>Start</name>
      <description/>
      <type>SPECIAL</type>
      <attributes/>
      <start>Y</start>
      <dummy>N</dummy>
      <repeat>N</repeat>
      <schedulerType>0</schedulerType>
      <intervalSeconds>0</intervalSeconds>
      <intervalMinutes>60</intervalMinutes>
      <hour>12</hour>
      <minutes>0</minutes>
      <weekDay>1</weekDay>
      <DayOfMonth>1</DayOfMonth>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>48</xloc>
      <yloc>112</yloc>
      <attributes_kjc/>
    </entry>
    <entry>
      <name>Set variables</name>
      <description/>
      <type>SET_VARIABLES</type>
      <attributes/>
      <replacevars>Y</replacevars>
      <filename/>
      <file_variable_type>JVM</file_variable_type>
      <fields>
        <field>
          <variable_name>DATASET</variable_name>
          <variable_value>sfp</variable_value>
          <variable_type>JVM</variable_type>
        </field>
        <field>
          <variable_name>HASH_TABLE</variable_name>
          <variable_value>staging.sources</variable_value>
          <variable_type>JVM</variable_type>
        </field>
        <field>
          <variable_name>DB_FOR_HASHES</variable_name>
          <variable_value>maindb</variable_value>
          <variable_type>JVM</variable_type>
        </field>
        <field>
          <variable_name>FILE_NAME</variable_name>
          <variable_value>funcionarios_${ANO}_${MES}.csv</variable_value>
          <variable_type>JVM</variable_type>
        </field>
        <field>
          <variable_name>FOLDER</variable_name>
          <variable_value>/tmp/sfp/</variable_value>
          <variable_type>JVM</variable_type>
        </field>
        <field>
          <variable_name>FTP_FOLDER</variable_name>
          <variable_value>./data/sfp/</variable_value>
          <variable_type>JVM</variable_type>
        </field>
        <field>
          <variable_name>URL</variable_name>
          <variable_value>https://datos.sfp.gov.py/data/funcionarios_${ANO}_${MES}.csv.zip</variable_value>
          <variable_type>JVM</variable_type>
        </field>
        <field>
          <variable_name>DOWNLOADED</variable_name>
          <variable_value>funcionarios_${ANO}_${MES}.zip</variable_value>
          <variable_type>JVM</variable_type>
        </field>
      </fields>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>48</xloc>
      <yloc>208</yloc>
      <attributes_kjc/>
    </entry>
    <entry>
      <name>Check hash</name>
      <description/>
      <type>EVAL</type>
      <attributes/>
      <script>var spoon = Packages.org.pentaho.di.core.gui.SpoonFactory.getInstance();

var folder = parent_job.getVariable("FOLDER");
var fileName = parent_job.getVariable("DOWNLOADED");

var test = org.pentaho.di.core.vfs.KettleVFS.getFileObject("file://" + folder + fileName, parent_job);

if (!test.exists()) {
	parent_job.stopAll()
}

var hasher = java.security.MessageDigest.getInstance("MD5");
var buffer = java.lang.reflect.Array.newInstance(java.lang.Byte.TYPE, 1024);
var is = test.getContent().getInputStream();

var numRead;
var test = 0;
do {
  numRead = is.read(buffer, 0, 1024);
  test += numRead;
  if (numRead > 0) {
      hasher.update(buffer, 0, 1);
  }
} while (numRead != -1);
is.close();
var hash = hasher.digest();




var asStr = java.lang.Long.toHexString(java.nio.ByteBuffer.wrap(hash).getLong());

parent_job.getLogChannel().logBasic("HASH: " + asStr);
parent_job.setVariable("FILE_PATH_HASH", asStr);

true</script>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>288</xloc>
      <yloc>208</yloc>
      <attributes_kjc/>
    </entry>
    <entry>
      <name>Fetch hash from db</name>
      <description/>
      <type>EVAL</type>
      <attributes/>
      <script>var databases = parent_job.getJobMeta().getDatabases();

var ref = Packages.org.pentaho.di.core.database.DatabaseMeta.findDatabase(databases, parent_job.getVariable("DB_FOR_HASHES"));


var db = new Packages.org.pentaho.di.core.database.Database(
	parent_job, ref
);

var hashTable = parent_job.getVariable("HASH_TABLE");
var source = parent_job.getVariable("DATASET");
var WHERE = "dataset = '" + source + "'";
var FILTER_BY_FILE = parent_job.getVariable("FILE_NAME");


var query = "SELECT hash FROM " + hashTable + " WHERE " + WHERE + " ORDER BY id DESC LIMIT 1"

db.connect();
parent_job.getLogChannel().logBasic("QUERY: " + query);


var rs = db.openQuery(query); 
var hash = '-1';
if (rs.next()) {
	hash = rs.getString(1);
}
db.disconnect();

parent_job.setVariable("DB_HASH", hash);
parent_job.getLogChannel().logBasic("DB HASH: " + hash);

true</script>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>656</xloc>
      <yloc>208</yloc>
      <attributes_kjc/>
    </entry>
    <entry>
      <name>Check DB connections</name>
      <description/>
      <type>CHECK_DB_CONNECTIONS</type>
      <attributes/>
      <connections>
        <connection>
          <name>maindb</name>
          <waitfor>1000</waitfor>
          <waittime>millisecond</waittime>
        </connection>
      </connections>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>480</xloc>
      <yloc>208</yloc>
      <attributes_kjc/>
    </entry>
    <entry>
      <name>check if is needed to proceed</name>
      <description/>
      <type>EVAL</type>
      <attributes/>
      <script>var fileHash = parent_job.getVariable("FILE_PATH_HASH");
var dbHash = parent_job.getVariable("DB_HASH")

if (fileHash == dbHash) {
	parent_job.getLogChannel().logBasic("File already processed. ABORTING!!!!!!");
    parent_job.stopAll()
}

true</script>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>832</xloc>
      <yloc>208</yloc>
      <attributes_kjc/>
    </entry>
    <entry>
      <name>Put a file with FTP</name>
      <description/>
      <type>FTP_PUT</type>
      <attributes/>
      <servername>datapy.ftp.cds.com.py</servername>
      <serverport>21</serverport>
      <username>ftpuser</username>
      <password>Encrypted 73757067ccb8c3a918c097c4ad0dbe30ce93bcc9</password>
      <remoteDirectory>${FTP_FOLDER}</remoteDirectory>
      <localDirectory>${FOLDER}</localDirectory>
      <wildcard>${FILE_PATH_HASH}_${DOWNLOADED}</wildcard>
      <binary>N</binary>
      <timeout>0</timeout>
      <remove>N</remove>
      <only_new>N</only_new>
      <active>N</active>
      <control_encoding>ISO-8859-1</control_encoding>
      <proxy_host/>
      <proxy_port/>
      <proxy_username/>
      <proxy_password>Encrypted </proxy_password>
      <socksproxy_host/>
      <socksproxy_port>1080</socksproxy_port>
      <socksproxy_username/>
      <socksproxy_password>Encrypted </socksproxy_password>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>496</xloc>
      <yloc>528</yloc>
      <attributes_kjc/>
    </entry>
    <entry>
      <name>insert result to db</name>
      <description/>
      <type>EVAL</type>
      <attributes/>
      <script>var databases = parent_job.getJobMeta().getDatabases();

var ref = Packages.org.pentaho.di.core.database.DatabaseMeta.findDatabase(databases, parent_job.getVariable("DB_FOR_HASHES"));


var db = new Packages.org.pentaho.di.core.database.Database(
	parent_job, ref
);

var hashTable = parent_job.getVariable("HASH_TABLE");
var source = parent_job.getVariable("DATASET");
var WHERE = "dataset = '" + source + "'";
var FILTER_BY_FILE = parent_job.getVariable("FILE_NAME");
var localHash = parent_job.getVariable("FILE_PATH_HASH");
var original_uri = parent_job.getVariable("URL");

var QUERY_INSERT = "INSERT INTO " + hashTable;
var COLUMNS = " (file_name, dataset, hash, original_uri) ";
var VALUES = " VALUES ('" + parent_job.getVariable("FILE_NAME") + "', '" + source + "', '" + localHash + "', '" + original_uri + "');"; 
var query = QUERY_INSERT + COLUMNS + VALUES;

db.connect();
parent_job.getLogChannel().logBasic("QUERY: " + query);


var ps = db.prepareSQL(query); 

var inserts = ps.executeUpdate();

parent_job.getLogChannel().logBasic("Hash sent, number of inserts: " + inserts);

db.disconnect();

true</script>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>1</nr>
      <xloc>672</xloc>
      <yloc>528</yloc>
      <attributes_kjc/>
    </entry>
    <entry>
      <name>copy file to prepend hash</name>
      <description/>
      <type>SHELL</type>
      <attributes/>
      <filename/>
      <work_directory/>
      <arg_from_previous>N</arg_from_previous>
      <exec_per_row>N</exec_per_row>
      <set_logfile>N</set_logfile>
      <logfile/>
      <set_append_logfile>N</set_append_logfile>
      <logext/>
      <add_date>N</add_date>
      <add_time>N</add_time>
      <insertScript>Y</insertScript>
      <script>set -x

cp "${FOLDER}${DOWNLOADED}" "${FOLDER}${FILE_PATH_HASH}_${DOWNLOADED}"</script>
      <loglevel>Basic</loglevel>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>832</xloc>
      <yloc>352</yloc>
      <attributes_kjc/>
    </entry>
    <entry>
      <name>sfp</name>
      <description/>
      <type>TRANS</type>
      <attributes/>
      <specification_method>filename</specification_method>
      <trans_object_id/>
      <filename>${Internal.Entry.Current.Directory}/sfp.ktr</filename>
      <transname/>
      <arg_from_previous>N</arg_from_previous>
      <params_from_previous>N</params_from_previous>
      <exec_per_row>N</exec_per_row>
      <clear_rows>N</clear_rows>
      <clear_files>N</clear_files>
      <set_logfile>Y</set_logfile>
      <logfile>/tmp/pentaho</logfile>
      <logext>log</logext>
      <add_date>N</add_date>
      <add_time>Y</add_time>
      <loglevel>Debug</loglevel>
      <cluster>N</cluster>
      <slave_server_name/>
      <set_append_logfile>Y</set_append_logfile>
      <wait_until_finished>Y</wait_until_finished>
      <follow_abort_remote>N</follow_abort_remote>
      <create_parent_folder>N</create_parent_folder>
      <logging_remote_work>N</logging_remote_work>
      <run_configuration>Pentaho local</run_configuration>
      <parameters>
        <pass_all_parameters>Y</pass_all_parameters>
        <parameter>
          <name>FILE</name>
          <stream_name/>
          <value>${FOLDER}${FILE_NAME}</value>
        </parameter>
      </parameters>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>832</xloc>
      <yloc>528</yloc>
      <attributes_kjc/>
    </entry>
    <entry>
      <name>HTTP</name>
      <description/>
      <type>HTTP</type>
      <attributes/>
      <url>${URL}</url>
      <targetfilename>${FOLDER}${DOWNLOADED}</targetfilename>
      <file_appended>N</file_appended>
      <date_time_added>N</date_time_added>
      <targetfilename_extension/>
      <uploadfilename/>
      <run_every_row>N</run_every_row>
      <url_fieldname/>
      <upload_fieldname/>
      <dest_fieldname/>
      <username/>
      <password>Encrypted </password>
      <proxy_host/>
      <proxy_port/>
      <non_proxy_hosts/>
      <addfilenameresult>Y</addfilenameresult>
      <headers>
      </headers>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>48</xloc>
      <yloc>576</yloc>
      <attributes_kjc/>
    </entry>
    <entry>
      <name>Unzip file</name>
      <description/>
      <type>UNZIP</type>
      <attributes/>
      <zipfilename>${FOLDER}${DOWNLOADED}</zipfilename>
      <wildcard/>
      <wildcardexclude/>
      <targetdirectory>${FOLDER}</targetdirectory>
      <movetodirectory/>
      <afterunzip>0</afterunzip>
      <addfiletoresult>N</addfiletoresult>
      <isfromprevious>N</isfromprevious>
      <adddate>N</adddate>
      <addtime>N</addtime>
      <addOriginalTimestamp>N</addOriginalTimestamp>
      <SpecifyFormat>N</SpecifyFormat>
      <date_time_format/>
      <rootzip>N</rootzip>
      <createfolder>N</createfolder>
      <nr_limit>10</nr_limit>
      <wildcardSource/>
      <success_condition>success_if_no_errors</success_condition>
      <iffileexists>SKIP</iffileexists>
      <create_move_to_directory>N</create_move_to_directory>
      <setOriginalModificationDate>N</setOriginalModificationDate>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>288</xloc>
      <yloc>576</yloc>
      <attributes_kjc/>
    </entry>
    <entry>
      <name>create target folder</name>
      <description/>
      <type>SHELL</type>
      <attributes/>
      <filename/>
      <work_directory/>
      <arg_from_previous>N</arg_from_previous>
      <exec_per_row>N</exec_per_row>
      <set_logfile>N</set_logfile>
      <logfile/>
      <set_append_logfile>N</set_append_logfile>
      <logext/>
      <add_date>N</add_date>
      <add_time>N</add_time>
      <insertScript>Y</insertScript>
      <script>mkdir -p "${FOLDER}"

export</script>
      <loglevel>Basic</loglevel>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>48</xloc>
      <yloc>320</yloc>
      <attributes_kjc/>
    </entry>
    <entry>
      <name>File exists</name>
      <description/>
      <type>FILE_EXISTS</type>
      <attributes/>
      <filename>${FOLDER}${DOWNLOADED}</filename>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>48</xloc>
      <yloc>448</yloc>
      <attributes_kjc/>
    </entry>
  </entries>
  <hops>
    <hop>
      <from>Start</from>
      <to>Set variables</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>Y</unconditional>
    </hop>
    <hop>
      <from>Check hash</from>
      <to>Check DB connections</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>Check DB connections</from>
      <to>Fetch hash from db</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>Fetch hash from db</from>
      <to>check if is needed to proceed</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>Set variables</from>
      <to>create target folder</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>HTTP</from>
      <to>Unzip file</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>Unzip file</from>
      <to>Check hash</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>check if is needed to proceed</from>
      <to>copy file to prepend hash</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>copy file to prepend hash</from>
      <to>sfp</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>sfp</from>
      <to>insert result to db</to>
      <from_nr>0</from_nr>
      <to_nr>1</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>insert result to db</from>
      <to>Put a file with FTP</to>
      <from_nr>1</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>create target folder</from>
      <to>File exists</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>File exists</from>
      <to>Unzip file</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>File exists</from>
      <to>HTTP</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>N</evaluation>
      <unconditional>N</unconditional>
    </hop>
  </hops>
  <notepads>
  </notepads>
  <attributes/>
</job>
